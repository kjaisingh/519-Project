{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "519-Project-KJ.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCNlJQHz-lzq"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLwRV7HRRx5-"
      },
      "source": [
        "## Naive Model Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIwX8IOUG3VH",
        "outputId": "b5367665-3199-4d66-ab87-4ef5ed5e1014"
      },
      "source": [
        "!pip install scikit-learn==0.21.2\n",
        "!pip install bioinfokit"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (0.21.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2) (1.4.1)\n",
            "Requirement already satisfied: bioinfokit in /usr/local/lib/python3.7/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (3.2.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.10.2)\n",
            "Requirement already satisfied: textwrap3 in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.9.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.21.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.8.9)\n",
            "Requirement already satisfied: adjustText in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.7.3)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.11.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from bioinfokit) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bioinfokit) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bioinfokit) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bioinfokit) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bioinfokit) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->bioinfokit) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bioinfokit) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->bioinfokit) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->bioinfokit) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZGYi-Gcqbg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import initializers\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYMb-d01R1Ak"
      },
      "source": [
        "## Model Optimization Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZCV-9ohRbFo"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIe_NkhhR3Yi"
      },
      "source": [
        "## Further Experiment Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW0fTblYRfo7"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from bioinfokit.visuz import cluster"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4oXjPZ8EhmD"
      },
      "source": [
        "## Metadata Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAy64-Xk-oIy"
      },
      "source": [
        "btc_meta = pd.read_csv('Bitcoin-Metadata.csv')\n",
        "btc_meta = btc_meta.drop(['SNo', 'Name', 'Symbol', 'Date'], axis = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "4yJxgwvyfY3o",
        "outputId": "39c80ebd-9873-45a2-981c-510ecd962732"
      },
      "source": [
        "btc_meta.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Marketcap</th>\n",
              "      <th>Day_Diff</th>\n",
              "      <th>Rel_Close</th>\n",
              "      <th>HL_Ratio</th>\n",
              "      <th>Rel_High</th>\n",
              "      <th>Rel_Low</th>\n",
              "      <th>SMA7</th>\n",
              "      <th>SMA30</th>\n",
              "      <th>SMA60</th>\n",
              "      <th>SMA90</th>\n",
              "      <th>SMA200</th>\n",
              "      <th>CMA</th>\n",
              "      <th>EMA1</th>\n",
              "      <th>EMA2</th>\n",
              "      <th>EMA3</th>\n",
              "      <th>EMA4</th>\n",
              "      <th>EMA5</th>\n",
              "      <th>EMA6</th>\n",
              "      <th>EMA7</th>\n",
              "      <th>EMA8</th>\n",
              "      <th>EMA9</th>\n",
              "      <th>EMA10</th>\n",
              "      <th>Close2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>425.899994</td>\n",
              "      <td>395.190002</td>\n",
              "      <td>406.410004</td>\n",
              "      <td>420.200012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.038818e+09</td>\n",
              "      <td>13.790009</td>\n",
              "      <td>0.033931</td>\n",
              "      <td>1.077709</td>\n",
              "      <td>1.013565</td>\n",
              "      <td>0.940481</td>\n",
              "      <td>362.025709</td>\n",
              "      <td>238.061666</td>\n",
              "      <td>184.322333</td>\n",
              "      <td>164.856867</td>\n",
              "      <td>132.887528</td>\n",
              "      <td>132.887528</td>\n",
              "      <td>294.314803</td>\n",
              "      <td>347.211757</td>\n",
              "      <td>373.225716</td>\n",
              "      <td>388.494797</td>\n",
              "      <td>398.657691</td>\n",
              "      <td>406.017945</td>\n",
              "      <td>411.524047</td>\n",
              "      <td>415.587278</td>\n",
              "      <td>418.427258</td>\n",
              "      <td>420.200012</td>\n",
              "      <td>417.950012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>437.890015</td>\n",
              "      <td>396.109985</td>\n",
              "      <td>419.410004</td>\n",
              "      <td>417.950012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.013561e+09</td>\n",
              "      <td>-1.459991</td>\n",
              "      <td>-0.003481</td>\n",
              "      <td>1.105476</td>\n",
              "      <td>1.047709</td>\n",
              "      <td>0.947745</td>\n",
              "      <td>373.431427</td>\n",
              "      <td>247.241667</td>\n",
              "      <td>189.092833</td>\n",
              "      <td>168.239645</td>\n",
              "      <td>134.254578</td>\n",
              "      <td>134.305749</td>\n",
              "      <td>306.678324</td>\n",
              "      <td>361.359408</td>\n",
              "      <td>386.643005</td>\n",
              "      <td>400.276883</td>\n",
              "      <td>408.303852</td>\n",
              "      <td>413.177185</td>\n",
              "      <td>416.022223</td>\n",
              "      <td>417.477465</td>\n",
              "      <td>417.997737</td>\n",
              "      <td>417.950012</td>\n",
              "      <td>440.220001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450.260010</td>\n",
              "      <td>415.570007</td>\n",
              "      <td>417.279999</td>\n",
              "      <td>440.220001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.282849e+09</td>\n",
              "      <td>22.940002</td>\n",
              "      <td>0.054975</td>\n",
              "      <td>1.083476</td>\n",
              "      <td>1.022807</td>\n",
              "      <td>0.944005</td>\n",
              "      <td>387.875715</td>\n",
              "      <td>257.040667</td>\n",
              "      <td>194.235500</td>\n",
              "      <td>171.869867</td>\n",
              "      <td>135.760678</td>\n",
              "      <td>135.820176</td>\n",
              "      <td>320.032492</td>\n",
              "      <td>377.131527</td>\n",
              "      <td>402.716104</td>\n",
              "      <td>416.254131</td>\n",
              "      <td>424.261927</td>\n",
              "      <td>429.402875</td>\n",
              "      <td>432.960668</td>\n",
              "      <td>435.671494</td>\n",
              "      <td>437.997775</td>\n",
              "      <td>440.220001</td>\n",
              "      <td>492.109985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>500.579987</td>\n",
              "      <td>440.239990</td>\n",
              "      <td>440.959991</td>\n",
              "      <td>492.109985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.907842e+09</td>\n",
              "      <td>51.149994</td>\n",
              "      <td>0.115997</td>\n",
              "      <td>1.137062</td>\n",
              "      <td>1.017212</td>\n",
              "      <td>0.894597</td>\n",
              "      <td>411.517142</td>\n",
              "      <td>268.245666</td>\n",
              "      <td>200.246166</td>\n",
              "      <td>176.015533</td>\n",
              "      <td>137.636278</td>\n",
              "      <td>137.575298</td>\n",
              "      <td>337.240241</td>\n",
              "      <td>400.127218</td>\n",
              "      <td>429.534268</td>\n",
              "      <td>446.596472</td>\n",
              "      <td>458.185956</td>\n",
              "      <td>467.027141</td>\n",
              "      <td>474.365190</td>\n",
              "      <td>480.822287</td>\n",
              "      <td>486.698764</td>\n",
              "      <td>492.109985</td>\n",
              "      <td>703.559998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>703.780029</td>\n",
              "      <td>494.940002</td>\n",
              "      <td>496.579987</td>\n",
              "      <td>703.559998</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.449070e+09</td>\n",
              "      <td>206.980011</td>\n",
              "      <td>0.416811</td>\n",
              "      <td>1.421950</td>\n",
              "      <td>1.000313</td>\n",
              "      <td>0.703479</td>\n",
              "      <td>463.105713</td>\n",
              "      <td>285.950333</td>\n",
              "      <td>209.811333</td>\n",
              "      <td>182.486145</td>\n",
              "      <td>140.628028</td>\n",
              "      <td>140.349733</td>\n",
              "      <td>373.872217</td>\n",
              "      <td>460.813774</td>\n",
              "      <td>511.741987</td>\n",
              "      <td>549.381882</td>\n",
              "      <td>580.872977</td>\n",
              "      <td>608.946855</td>\n",
              "      <td>634.801555</td>\n",
              "      <td>659.012455</td>\n",
              "      <td>681.873874</td>\n",
              "      <td>703.559998</td>\n",
              "      <td>584.609985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         High         Low        Open  ...        EMA9       EMA10      Close2\n",
              "0  425.899994  395.190002  406.410004  ...  418.427258  420.200012  417.950012\n",
              "1  437.890015  396.109985  419.410004  ...  417.997737  417.950012  440.220001\n",
              "2  450.260010  415.570007  417.279999  ...  437.997775  440.220001  492.109985\n",
              "3  500.579987  440.239990  440.959991  ...  486.698764  492.109985  703.559998\n",
              "4  703.780029  494.940002  496.579987  ...  681.873874  703.559998  584.609985\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BfpQb-whA9P"
      },
      "source": [
        "def f(x):\n",
        "  if (x['Close2'] > x['Close']):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "btc_meta_c = btc_meta.copy()\n",
        "btc_meta_c['Direction'] = btc_meta_c.apply(f, axis = 1)\n",
        "btc_meta_c = btc_meta_c.drop(btc_meta_c.columns[-2], axis = 1)\n",
        "\n",
        "btc_meta_r = btc_meta"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbX6qZoDElKu"
      },
      "source": [
        "## Historical Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i79cAsQLEfEa"
      },
      "source": [
        "btc_hist = pd.read_csv('Bitcoin-Historical.csv')\n",
        "btc_hist = btc_hist.drop(['SNo', 'Date'], axis = 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "imvgg5gIEwP-",
        "outputId": "bd127f09-d9f5-45b8-b52b-ff96f653a737"
      },
      "source": [
        "btc_hist.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close1</th>\n",
              "      <th>Close2</th>\n",
              "      <th>Close3</th>\n",
              "      <th>Close4</th>\n",
              "      <th>Close5</th>\n",
              "      <th>Close6</th>\n",
              "      <th>Close7</th>\n",
              "      <th>Close8</th>\n",
              "      <th>Close9</th>\n",
              "      <th>Close10</th>\n",
              "      <th>Close11</th>\n",
              "      <th>Close12</th>\n",
              "      <th>Close13</th>\n",
              "      <th>Close14</th>\n",
              "      <th>Close15</th>\n",
              "      <th>Close16</th>\n",
              "      <th>Close17</th>\n",
              "      <th>Close18</th>\n",
              "      <th>Close19</th>\n",
              "      <th>Close20</th>\n",
              "      <th>Close21</th>\n",
              "      <th>Close22</th>\n",
              "      <th>Close23</th>\n",
              "      <th>Close24</th>\n",
              "      <th>Close25</th>\n",
              "      <th>Close26</th>\n",
              "      <th>Close27</th>\n",
              "      <th>Close28</th>\n",
              "      <th>Close29</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close31</th>\n",
              "      <th>Close32</th>\n",
              "      <th>Close33</th>\n",
              "      <th>Close34</th>\n",
              "      <th>Close35</th>\n",
              "      <th>Close36</th>\n",
              "      <th>Close37</th>\n",
              "      <th>Close38</th>\n",
              "      <th>Close39</th>\n",
              "      <th>Close40</th>\n",
              "      <th>...</th>\n",
              "      <th>Close52</th>\n",
              "      <th>Close53</th>\n",
              "      <th>Close54</th>\n",
              "      <th>Close55</th>\n",
              "      <th>Close56</th>\n",
              "      <th>Close57</th>\n",
              "      <th>Close58</th>\n",
              "      <th>Close59</th>\n",
              "      <th>Close60</th>\n",
              "      <th>Close61</th>\n",
              "      <th>Close62</th>\n",
              "      <th>Close63</th>\n",
              "      <th>Close64</th>\n",
              "      <th>Close65</th>\n",
              "      <th>Close66</th>\n",
              "      <th>Close67</th>\n",
              "      <th>Close68</th>\n",
              "      <th>Close69</th>\n",
              "      <th>Close70</th>\n",
              "      <th>Close71</th>\n",
              "      <th>Close72</th>\n",
              "      <th>Close73</th>\n",
              "      <th>Close74</th>\n",
              "      <th>Close75</th>\n",
              "      <th>Close76</th>\n",
              "      <th>Close77</th>\n",
              "      <th>Close78</th>\n",
              "      <th>Close79</th>\n",
              "      <th>Close80</th>\n",
              "      <th>Close81</th>\n",
              "      <th>Close82</th>\n",
              "      <th>Close83</th>\n",
              "      <th>Close84</th>\n",
              "      <th>Close85</th>\n",
              "      <th>Close86</th>\n",
              "      <th>Close87</th>\n",
              "      <th>Close88</th>\n",
              "      <th>Close89</th>\n",
              "      <th>Close90</th>\n",
              "      <th>Close91</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>144.539993</td>\n",
              "      <td>139.000000</td>\n",
              "      <td>116.989998</td>\n",
              "      <td>105.209999</td>\n",
              "      <td>97.750000</td>\n",
              "      <td>112.500000</td>\n",
              "      <td>115.910004</td>\n",
              "      <td>112.300003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>113.566002</td>\n",
              "      <td>112.669998</td>\n",
              "      <td>117.199997</td>\n",
              "      <td>115.242996</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>117.980003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>114.220001</td>\n",
              "      <td>118.760002</td>\n",
              "      <td>123.014999</td>\n",
              "      <td>123.498001</td>\n",
              "      <td>121.989998</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>122.879997</td>\n",
              "      <td>123.889000</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>133.199997</td>\n",
              "      <td>131.979996</td>\n",
              "      <td>133.479996</td>\n",
              "      <td>129.744995</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>132.300003</td>\n",
              "      <td>128.798996</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>129.300003</td>\n",
              "      <td>122.292000</td>\n",
              "      <td>122.222000</td>\n",
              "      <td>121.419998</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>108.250000</td>\n",
              "      <td>110.150002</td>\n",
              "      <td>109.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>102.737000</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>101.436996</td>\n",
              "      <td>94.649200</td>\n",
              "      <td>94.994003</td>\n",
              "      <td>96.613998</td>\n",
              "      <td>88.050003</td>\n",
              "      <td>90.134003</td>\n",
              "      <td>77.529999</td>\n",
              "      <td>80.525803</td>\n",
              "      <td>68.431000</td>\n",
              "      <td>70.277298</td>\n",
              "      <td>74.561096</td>\n",
              "      <td>76.515999</td>\n",
              "      <td>76.694000</td>\n",
              "      <td>86.760002</td>\n",
              "      <td>88.980003</td>\n",
              "      <td>93.594902</td>\n",
              "      <td>98.133904</td>\n",
              "      <td>94.691299</td>\n",
              "      <td>98.400200</td>\n",
              "      <td>97.450798</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>90.580002</td>\n",
              "      <td>92.169998</td>\n",
              "      <td>89.390099</td>\n",
              "      <td>90.757301</td>\n",
              "      <td>91.610001</td>\n",
              "      <td>95.558502</td>\n",
              "      <td>94.510002</td>\n",
              "      <td>96.900002</td>\n",
              "      <td>96.020203</td>\n",
              "      <td>94.115997</td>\n",
              "      <td>99.755997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>139.000000</td>\n",
              "      <td>116.989998</td>\n",
              "      <td>105.209999</td>\n",
              "      <td>97.750000</td>\n",
              "      <td>112.500000</td>\n",
              "      <td>115.910004</td>\n",
              "      <td>112.300003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>113.566002</td>\n",
              "      <td>112.669998</td>\n",
              "      <td>117.199997</td>\n",
              "      <td>115.242996</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>117.980003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>114.220001</td>\n",
              "      <td>118.760002</td>\n",
              "      <td>123.014999</td>\n",
              "      <td>123.498001</td>\n",
              "      <td>121.989998</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>122.879997</td>\n",
              "      <td>123.889000</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>133.199997</td>\n",
              "      <td>131.979996</td>\n",
              "      <td>133.479996</td>\n",
              "      <td>129.744995</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>132.300003</td>\n",
              "      <td>128.798996</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>129.300003</td>\n",
              "      <td>122.292000</td>\n",
              "      <td>122.222000</td>\n",
              "      <td>121.419998</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>...</td>\n",
              "      <td>110.150002</td>\n",
              "      <td>109.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>102.737000</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>101.436996</td>\n",
              "      <td>94.649200</td>\n",
              "      <td>94.994003</td>\n",
              "      <td>96.613998</td>\n",
              "      <td>88.050003</td>\n",
              "      <td>90.134003</td>\n",
              "      <td>77.529999</td>\n",
              "      <td>80.525803</td>\n",
              "      <td>68.431000</td>\n",
              "      <td>70.277298</td>\n",
              "      <td>74.561096</td>\n",
              "      <td>76.515999</td>\n",
              "      <td>76.694000</td>\n",
              "      <td>86.760002</td>\n",
              "      <td>88.980003</td>\n",
              "      <td>93.594902</td>\n",
              "      <td>98.133904</td>\n",
              "      <td>94.691299</td>\n",
              "      <td>98.400200</td>\n",
              "      <td>97.450798</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>90.580002</td>\n",
              "      <td>92.169998</td>\n",
              "      <td>89.390099</td>\n",
              "      <td>90.757301</td>\n",
              "      <td>91.610001</td>\n",
              "      <td>95.558502</td>\n",
              "      <td>94.510002</td>\n",
              "      <td>96.900002</td>\n",
              "      <td>96.020203</td>\n",
              "      <td>94.115997</td>\n",
              "      <td>99.755997</td>\n",
              "      <td>101.199997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.989998</td>\n",
              "      <td>105.209999</td>\n",
              "      <td>97.750000</td>\n",
              "      <td>112.500000</td>\n",
              "      <td>115.910004</td>\n",
              "      <td>112.300003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>113.566002</td>\n",
              "      <td>112.669998</td>\n",
              "      <td>117.199997</td>\n",
              "      <td>115.242996</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>117.980003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>114.220001</td>\n",
              "      <td>118.760002</td>\n",
              "      <td>123.014999</td>\n",
              "      <td>123.498001</td>\n",
              "      <td>121.989998</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>122.879997</td>\n",
              "      <td>123.889000</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>133.199997</td>\n",
              "      <td>131.979996</td>\n",
              "      <td>133.479996</td>\n",
              "      <td>129.744995</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>132.300003</td>\n",
              "      <td>128.798996</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>129.300003</td>\n",
              "      <td>122.292000</td>\n",
              "      <td>122.222000</td>\n",
              "      <td>121.419998</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>109.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>102.737000</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>101.436996</td>\n",
              "      <td>94.649200</td>\n",
              "      <td>94.994003</td>\n",
              "      <td>96.613998</td>\n",
              "      <td>88.050003</td>\n",
              "      <td>90.134003</td>\n",
              "      <td>77.529999</td>\n",
              "      <td>80.525803</td>\n",
              "      <td>68.431000</td>\n",
              "      <td>70.277298</td>\n",
              "      <td>74.561096</td>\n",
              "      <td>76.515999</td>\n",
              "      <td>76.694000</td>\n",
              "      <td>86.760002</td>\n",
              "      <td>88.980003</td>\n",
              "      <td>93.594902</td>\n",
              "      <td>98.133904</td>\n",
              "      <td>94.691299</td>\n",
              "      <td>98.400200</td>\n",
              "      <td>97.450798</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>90.580002</td>\n",
              "      <td>92.169998</td>\n",
              "      <td>89.390099</td>\n",
              "      <td>90.757301</td>\n",
              "      <td>91.610001</td>\n",
              "      <td>95.558502</td>\n",
              "      <td>94.510002</td>\n",
              "      <td>96.900002</td>\n",
              "      <td>96.020203</td>\n",
              "      <td>94.115997</td>\n",
              "      <td>99.755997</td>\n",
              "      <td>101.199997</td>\n",
              "      <td>107.989998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>105.209999</td>\n",
              "      <td>97.750000</td>\n",
              "      <td>112.500000</td>\n",
              "      <td>115.910004</td>\n",
              "      <td>112.300003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>113.566002</td>\n",
              "      <td>112.669998</td>\n",
              "      <td>117.199997</td>\n",
              "      <td>115.242996</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>117.980003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>114.220001</td>\n",
              "      <td>118.760002</td>\n",
              "      <td>123.014999</td>\n",
              "      <td>123.498001</td>\n",
              "      <td>121.989998</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>122.879997</td>\n",
              "      <td>123.889000</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>133.199997</td>\n",
              "      <td>131.979996</td>\n",
              "      <td>133.479996</td>\n",
              "      <td>129.744995</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>132.300003</td>\n",
              "      <td>128.798996</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>129.300003</td>\n",
              "      <td>122.292000</td>\n",
              "      <td>122.222000</td>\n",
              "      <td>121.419998</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>106.349998</td>\n",
              "      <td>...</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>102.737000</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>101.436996</td>\n",
              "      <td>94.649200</td>\n",
              "      <td>94.994003</td>\n",
              "      <td>96.613998</td>\n",
              "      <td>88.050003</td>\n",
              "      <td>90.134003</td>\n",
              "      <td>77.529999</td>\n",
              "      <td>80.525803</td>\n",
              "      <td>68.431000</td>\n",
              "      <td>70.277298</td>\n",
              "      <td>74.561096</td>\n",
              "      <td>76.515999</td>\n",
              "      <td>76.694000</td>\n",
              "      <td>86.760002</td>\n",
              "      <td>88.980003</td>\n",
              "      <td>93.594902</td>\n",
              "      <td>98.133904</td>\n",
              "      <td>94.691299</td>\n",
              "      <td>98.400200</td>\n",
              "      <td>97.450798</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>90.580002</td>\n",
              "      <td>92.169998</td>\n",
              "      <td>89.390099</td>\n",
              "      <td>90.757301</td>\n",
              "      <td>91.610001</td>\n",
              "      <td>95.558502</td>\n",
              "      <td>94.510002</td>\n",
              "      <td>96.900002</td>\n",
              "      <td>96.020203</td>\n",
              "      <td>94.115997</td>\n",
              "      <td>99.755997</td>\n",
              "      <td>101.199997</td>\n",
              "      <td>107.989998</td>\n",
              "      <td>106.089996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>97.750000</td>\n",
              "      <td>112.500000</td>\n",
              "      <td>115.910004</td>\n",
              "      <td>112.300003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>113.566002</td>\n",
              "      <td>112.669998</td>\n",
              "      <td>117.199997</td>\n",
              "      <td>115.242996</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>117.980003</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>114.220001</td>\n",
              "      <td>118.760002</td>\n",
              "      <td>123.014999</td>\n",
              "      <td>123.498001</td>\n",
              "      <td>121.989998</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>122.879997</td>\n",
              "      <td>123.889000</td>\n",
              "      <td>126.699997</td>\n",
              "      <td>133.199997</td>\n",
              "      <td>131.979996</td>\n",
              "      <td>133.479996</td>\n",
              "      <td>129.744995</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>132.300003</td>\n",
              "      <td>128.798996</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>129.300003</td>\n",
              "      <td>122.292000</td>\n",
              "      <td>122.222000</td>\n",
              "      <td>121.419998</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>108.300003</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>106.349998</td>\n",
              "      <td>108.900002</td>\n",
              "      <td>...</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>102.737000</td>\n",
              "      <td>103.949997</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>101.436996</td>\n",
              "      <td>94.649200</td>\n",
              "      <td>94.994003</td>\n",
              "      <td>96.613998</td>\n",
              "      <td>88.050003</td>\n",
              "      <td>90.134003</td>\n",
              "      <td>77.529999</td>\n",
              "      <td>80.525803</td>\n",
              "      <td>68.431000</td>\n",
              "      <td>70.277298</td>\n",
              "      <td>74.561096</td>\n",
              "      <td>76.515999</td>\n",
              "      <td>76.694000</td>\n",
              "      <td>86.760002</td>\n",
              "      <td>88.980003</td>\n",
              "      <td>93.594902</td>\n",
              "      <td>98.133904</td>\n",
              "      <td>94.691299</td>\n",
              "      <td>98.400200</td>\n",
              "      <td>97.450798</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>90.580002</td>\n",
              "      <td>92.169998</td>\n",
              "      <td>89.390099</td>\n",
              "      <td>90.757301</td>\n",
              "      <td>91.610001</td>\n",
              "      <td>95.558502</td>\n",
              "      <td>94.510002</td>\n",
              "      <td>96.900002</td>\n",
              "      <td>96.020203</td>\n",
              "      <td>94.115997</td>\n",
              "      <td>99.755997</td>\n",
              "      <td>101.199997</td>\n",
              "      <td>107.989998</td>\n",
              "      <td>106.089996</td>\n",
              "      <td>104.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Close1      Close2      Close3  ...     Close89     Close90     Close91\n",
              "0  144.539993  139.000000  116.989998  ...   96.020203   94.115997   99.755997\n",
              "1  139.000000  116.989998  105.209999  ...   94.115997   99.755997  101.199997\n",
              "2  116.989998  105.209999   97.750000  ...   99.755997  101.199997  107.989998\n",
              "3  105.209999   97.750000  112.500000  ...  101.199997  107.989998  106.089996\n",
              "4   97.750000  112.500000  115.910004  ...  107.989998  106.089996  104.000000\n",
              "\n",
              "[5 rows x 91 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdT9nJXIEymh"
      },
      "source": [
        "def f(x):\n",
        "  if (x['Close91'] > x['Close90']):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "btc_hist_c = btc_hist.copy()\n",
        "btc_hist_c['Direction'] = btc_hist_c.apply(f, axis = 1)\n",
        "btc_hist_c = btc_hist_c.drop(btc_hist_c.columns[-2], axis = 1)\n",
        "\n",
        "btc_hist_r = btc_hist"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf4juROVcExT"
      },
      "source": [
        "# Naive Approach - Classification (Metadata)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d18-NpPcGSq"
      },
      "source": [
        "X = btc_meta_c.iloc[:, :-1]\n",
        "y = btc_meta_c.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJYp3h5cu9Xs"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrjh9UJYrk8V"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URi4fQAfrpVe"
      },
      "source": [
        "clf = SVC(kernel = 'linear', gamma = 'auto', cache_size = 7000, max_iter = 1e7)\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pG6ZqLgo7ga"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "svm_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mblhoaFrpfb"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq3bqHLCv8gv"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMWG6LQ-v8g5"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "rf_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69M2tRy7ruIN"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j57mQ3zr0Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd58b80-6c80-472e-cd36-8ed2448fd77d"
      },
      "source": [
        "clf = Sequential()\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform', \n",
        "                     activation = 'relu', input_dim = X_train.shape[1]))\n",
        "clf.add(Dropout(0.1))\n",
        "clf.add(Dense(units = 100, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "clf.add(Dropout(0.5))\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "clf.add(Dropout(0.1))\n",
        "clf.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "clf.fit(X_train, y_train, batch_size = (X_train.shape[0] // 10), epochs = 1000, verbose = 0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc79941a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtA_o0PayvIr",
        "outputId": "faa08259-a009-4f82-92b2-8d9a1455382c"
      },
      "source": [
        "y_pred = clf.predict_classes(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "nn_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8DQsStuzv3Z"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "lwlacus4zy4x",
        "outputId": "d7f118ca-859a-4111-8acf-3ae92153bed9"
      },
      "source": [
        "idx_rows = ['Support Vector Machine', 'Random Forest', 'Neural Network']\n",
        "data = {\n",
        "    'Accuracy':  [svm_metrics[0], rf_metrics[0], nn_metrics[0]],\n",
        "    'F-1 Score': [svm_metrics[1], rf_metrics[1], nn_metrics[1]]\n",
        "}\n",
        "results = pd.DataFrame(data, columns = ['Accuracy','F-1 Score'], index = idx_rows)\n",
        "results"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F-1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.678571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.543544</td>\n",
              "      <td>0.583562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>0.543544</td>\n",
              "      <td>0.610256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Accuracy  F-1 Score\n",
              "Support Vector Machine  0.513514   0.678571\n",
              "Random Forest           0.543544   0.583562\n",
              "Neural Network          0.543544   0.610256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQnnTobDcGcS"
      },
      "source": [
        "# Naive Approach - Regression (Metadata)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7anBK7Tr30D"
      },
      "source": [
        "X = btc_meta_r.iloc[:, :-1]\n",
        "y = btc_meta_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjVhTkZDGua6"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjIzpUnar30E"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vDNnVd3r30E"
      },
      "source": [
        "reg = SVR(kernel = 'linear', gamma = 'auto', cache_size = 7000, max_iter = 1e7)\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b8eu7D1HWMN"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "svm_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "               r2_score(y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjCRsjl4r30E"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awmBN80Hr30E"
      },
      "source": [
        "reg = RandomForestRegressor(n_estimators = 50, criterion = 'mse')\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A876OtmKEvK"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "rf_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alEjo55qr30F"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j23FjJer30F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f94ba8-a67d-49af-e978-fabe9435ba95"
      },
      "source": [
        "reg = Sequential()\n",
        "reg.add(Dense(units = 50, kernel_initializer = 'random_uniform', \n",
        "                     activation = 'relu', input_dim = X_train.shape[1]))\n",
        "reg.add(Dropout(0.1))\n",
        "reg.add(Dense(units = 100, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "reg.add(Dropout(0.5))\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "reg.add(Dropout(0.1))\n",
        "reg.add(Dense(units = 1))\n",
        "\n",
        "reg.compile(optimizer = 'adam', loss = 'mean_absolute_error')\n",
        "reg.fit(X_train, y_train, batch_size = (X_train.shape[0] // 10), epochs = 1000, verbose = 0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc796c6df50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC2NsoHP2gq6"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "nn_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR9HCKL02fMK"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "V3o03i16Kvbo",
        "outputId": "b79f5433-29c2-434c-fc69-59d301c040f4"
      },
      "source": [
        "idx_rows = ['Support Vector Machine', 'Random Forest', 'Neural Network']\n",
        "data = {\n",
        "    'MAE':  [svm_metrics[0], rf_metrics[0], nn_metrics[0]],\n",
        "    'R2 Score': [svm_metrics[1], rf_metrics[1], nn_metrics[1]]\n",
        "}\n",
        "results = pd.DataFrame(data, columns = ['MAE','R2 Score'], index = idx_rows)\n",
        "results"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <td>287.231899</td>\n",
              "      <td>0.988320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>211.994306</td>\n",
              "      <td>0.993369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>229.372334</td>\n",
              "      <td>0.993759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               MAE  R2 Score\n",
              "Support Vector Machine  287.231899  0.988320\n",
              "Random Forest           211.994306  0.993369\n",
              "Neural Network          229.372334  0.993759"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMtIXMO1fjV"
      },
      "source": [
        "# Naive Approach - Classification (Historical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fBh_ohH1fje"
      },
      "source": [
        "X = btc_hist_c.iloc[:, :-1]\n",
        "y = btc_hist_c.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZPkWB3h2CLq"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFve_fsv1fjf"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opzzKIoN1fjf"
      },
      "source": [
        "clf = SVC(kernel = 'linear', gamma = 'auto', cache_size = 7000, max_iter = 1e7)\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Cs8F2E1fjf"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "svm_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1wktDZP1fjg"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ7C8QyC1fjg"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')\n",
        "clf = clf.fit(X_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT3lXy-k1fjg"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "rf_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZJNZLC_1fjh"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vflt79qS1fjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66108891-5af7-4db1-a83b-2ac1676a8ee4"
      },
      "source": [
        "clf = Sequential()\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform', \n",
        "                     activation = 'relu', input_dim = X_train.shape[1]))\n",
        "clf.add(Dropout(0.1))\n",
        "clf.add(Dense(units = 100, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "clf.add(Dropout(0.5))\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "clf.add(Dropout(0.1))\n",
        "clf.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "clf.fit(X_train, y_train, batch_size = (X_train.shape[0] // 10), epochs = 1000, verbose = 0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7952c22d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yzGgMCj1fjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44fd710-f63c-407c-e61a-c52ba65ecb3f"
      },
      "source": [
        "y_pred = clf.predict_classes(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "nn_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8bED8fZ1fjh"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVLf5ULV1fjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "22ab4810-1998-4983-c7ed-b0aeb93ab072"
      },
      "source": [
        "idx_rows = ['Support Vector Machine', 'Random Forest', 'Neural Network']\n",
        "data = {\n",
        "    'Accuracy':  [svm_metrics[0], rf_metrics[0], nn_metrics[0]],\n",
        "    'F-1 Score': [svm_metrics[1], rf_metrics[1], nn_metrics[1]]\n",
        "}\n",
        "results = pd.DataFrame(data, columns = ['Accuracy','F-1 Score'], index = idx_rows)\n",
        "results"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F-1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <td>0.518038</td>\n",
              "      <td>0.667331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.509380</td>\n",
              "      <td>0.554974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>0.490620</td>\n",
              "      <td>0.615887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Accuracy  F-1 Score\n",
              "Support Vector Machine  0.518038   0.667331\n",
              "Random Forest           0.509380   0.554974\n",
              "Neural Network          0.490620   0.615887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFs2HWMV1fji"
      },
      "source": [
        "# Naive Approach - Regression (Historical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpye_ot2L7Lf"
      },
      "source": [
        "X = btc_hist_r.iloc[:, :-1]\n",
        "y = btc_hist_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd7Zf7qML7Lg"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x48Hd2D_L3G1"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsS1n8xQL3G2"
      },
      "source": [
        "reg = SVR(kernel = 'linear', gamma = 'auto', cache_size = 7000, max_iter = 1e7)\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96TS2cihL3G2"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "svm_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "               r2_score(y_test, y_pred))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS_QGC1ZL3G2"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DzafmE-L3G3"
      },
      "source": [
        "reg = RandomForestRegressor(n_estimators = 50, criterion = 'mse')\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L17YNCd2L3G3"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "rf_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Hi2CpNL3G3"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "455fLj71L3G3",
        "outputId": "c95eb088-84fb-4357-ff3a-493122541050"
      },
      "source": [
        "reg = Sequential()\n",
        "reg.add(Dense(units = 50, kernel_initializer = 'random_uniform', \n",
        "                     activation = 'relu', input_dim = X_train.shape[1]))\n",
        "reg.add(Dropout(0.1))\n",
        "reg.add(Dense(units = 100, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "reg.add(Dropout(0.5))\n",
        "clf.add(Dense(units = 50, kernel_initializer = 'random_uniform',\n",
        "                     activation = 'relu'))\n",
        "reg.add(Dropout(0.1))\n",
        "reg.add(Dense(units = 1))\n",
        "\n",
        "reg.compile(optimizer = 'adam', loss = 'mean_absolute_error')\n",
        "reg.fit(X_train, y_train, batch_size = (X_train.shape[0] // 10), epochs = 1000, verbose = 0)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc79412cdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6fq4DcsL3G4"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "nn_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byj1HXKeL3G4"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "LAkvcPqDL3G5",
        "outputId": "74f1295d-cf28-452f-fcde-00529646b2e4"
      },
      "source": [
        "idx_rows = ['Support Vector Machine', 'Random Forest', 'Neural Network']\n",
        "data = {\n",
        "    'MAE':  [svm_metrics[0], rf_metrics[0], nn_metrics[0]],\n",
        "    'R2 Score': [svm_metrics[1], rf_metrics[1], nn_metrics[1]]\n",
        "}\n",
        "results = pd.DataFrame(data, columns = ['MAE','R2 Score'], index = idx_rows)\n",
        "results"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <td>521.708092</td>\n",
              "      <td>0.965217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>159.783317</td>\n",
              "      <td>0.996039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>234.502067</td>\n",
              "      <td>0.994371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               MAE  R2 Score\n",
              "Support Vector Machine  521.708092  0.965217\n",
              "Random Forest           159.783317  0.996039\n",
              "Neural Network          234.502067  0.994371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYLZwd3jMSn1"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V21pG-ZM4NU"
      },
      "source": [
        "Now that we have baseline metric scores for a range of problems that we have been exploring, we can begin to optimise these naive models in order to test their limit. \n",
        "\n",
        "From the previous sections, we can see that all baseline models demonstrated some degree of competency. There was, however, one exception - the classification models that utilised historical closing price data to predict an upward or downward movement in next-day price. Here, no model could predict much above an accuracy 50% (the approximate random guessing percentage), so we do not proceed with optimising models to do so as it seems to be a problem that cannot be solved by such naive models. This makes sense logically - as naive models do not have a sense of the sequence of data, and there is no standard pattern in upward-downward price movement between particular days in the span of previous 90 days. \n",
        "\n",
        "We will also abstain from optimising the regression models trained on the metadata dataset - for this, the optimal models were found to be the Random Forest and Neural Network models. Hence, should we wish to optimise this model in the future, we can simply either of the optimisation techniques demonstrated in this section, as they cover each of these respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSRF0BguMXM2"
      },
      "source": [
        "## Classification (Metadata) - Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNivkCPvh5kY"
      },
      "source": [
        "The baseline Neural Network model outperformed the other models for this problem - hence, we can attempt to create an optized model for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmwNPErij2w-"
      },
      "source": [
        "X = btc_meta_c.iloc[:, :-1]\n",
        "y = btc_meta_c.iloc[:, -1]\n",
        "X = SelectKBest(score_func = f_classif, k = 10).fit_transform(X, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0F05_Uxj2w_"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8LqB4FnjweI",
        "outputId": "9c0be285-53a3-4737-f295-7ec16d348c9a"
      },
      "source": [
        "def build_classifier(optimizer = 'adam'):\n",
        "    clf = Sequential()\n",
        "    clf.add(Dense(units = 50, kernel_initializer = 'random_uniform', \n",
        "                        activation = 'relu', input_dim = X_train.shape[1]))\n",
        "    clf.add(Dropout(0.1))\n",
        "    clf.add(Dense(units = 100, kernel_initializer = 'random_uniform',\n",
        "                        activation = 'relu'))\n",
        "    clf.add(Dropout(0.5))\n",
        "    clf.add(Dense(units = 50, kernel_initializer = 'random_uniform',\n",
        "                        activation = 'relu'))\n",
        "    clf.add(Dropout(0.1))\n",
        "    clf.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "    clf.compile(optimizer = optimizer, loss = 'binary_crossentropy', \n",
        "                metrics = ['accuracy'])\n",
        "    return clf\n",
        "\n",
        "clf = KerasClassifier(build_fn = build_classifier, verbose = 0)\n",
        "\n",
        "batch_size = [10, 50]\n",
        "epochs = [100, 1000, 5000]\n",
        "optimizers = ['adam', 'adagrad', 'rmsprop']\n",
        "es = EarlyStopping(monitor = 'accuracy', patience = 25)\n",
        "callbacks = [es]\n",
        "\n",
        "grid = {'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'optimizer': optimizers,\n",
        "        'callbacks': callbacks}\n",
        "validator = GridSearchCV(clf, param_grid = grid, cv = 5,\n",
        "                         scoring = 'accuracy', n_jobs = 1, verbose = 0)\n",
        "validator.fit(X_train, y_train)\n",
        "clf = validator.best_estimator_.model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgXAGm-Dj_rc",
        "outputId": "00909076-2069-4eea-dade-022468c734cf"
      },
      "source": [
        "y_pred = clf.predict_classes(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "nn_metrics = (accuracy_score(y_test, y_pred), \n",
        "               f1_score(y_test, y_pred, average = 'binary'))\n",
        "nn_metrics"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5570570570570571, 0.7155255544840887)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZH_I6tv3vB8"
      },
      "source": [
        "clf_metadata_opt = clf"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnerVD_xMgiM"
      },
      "source": [
        "## Regression (Historical) - Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXj1WpvMiGIu"
      },
      "source": [
        "The baseline Random Forest model outperformed the other trialled models for this problem - hence, we can attempt to create an optized model for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ZKiZIE4EZ6"
      },
      "source": [
        "X = btc_hist_r.iloc[:, :-1]\n",
        "y = btc_hist_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzRJ22r14EaE"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GCE5LbMf_U"
      },
      "source": [
        "estimators = [50, 100, 200]\n",
        "depth = [None, 25, 50]\n",
        "features = ['auto', 'sqrt']\n",
        "\n",
        "grid = {'n_estimators': estimators,\n",
        "        'max_depth': depth,\n",
        "        'max_features': features}\n",
        "reg = RandomForestRegressor()\n",
        "validator = GridSearchCV(estimator = reg, param_grid = grid, \n",
        "                          cv = 5, n_jobs = 1, verbose = 0)\n",
        "validator.fit(X_train, y_train)\n",
        "reg = validator.best_estimator_\n",
        "reg_params = validator.best_params_"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtwLsCzj6FJc",
        "outputId": "35d2ccd7-5eeb-48d8-c31d-4d1278a1b8f5"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "rf_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))\n",
        "rf_metrics"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160.85777114612387, 0.9964980615048042)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJrTMFsD6GDC"
      },
      "source": [
        "reg_historical_opt = reg"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdS9qsg3PXvv"
      },
      "source": [
        "# Further Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gjRi85PbUr"
      },
      "source": [
        "## Reducing the N-Day Timeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtEwCsLnQDwm"
      },
      "source": [
        "Thus far, we have used the previous 90 days to predict the present day closing price of bitcoin. We can now test how our optimised model does when using a varying number of days to predict the present day closing price - 3 days, 10 days and 30 days. We will not change any aspect of the optimised model besides the number of input features it takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4JWGDnuPZCv",
        "outputId": "14fd972d-7b6b-4eab-957e-e1da4e2b7a4d"
      },
      "source": [
        "print(reg_params)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 50, 'max_features': 'sqrt', 'n_estimators': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3xnV7kv-H_a"
      },
      "source": [
        "X = btc_hist_r.iloc[:, :-1]\n",
        "y = btc_hist_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adwFTavC-H_b"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdAUnd5m-JxU"
      },
      "source": [
        "metrics = []\n",
        "days = [3, 10, 30, 90]\n",
        "\n",
        "for i in days:\n",
        "  X_train = X_train[:, -i:]\n",
        "  X_test = X_test[:, -i:]\n",
        "\n",
        "  reg = RandomForestRegressor(n_estimators = 200, max_depth = None,\n",
        "                              max_features = 'sqrt')\n",
        "  reg = reg.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = reg.predict(X_test)\n",
        "  m = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))\n",
        "  metrics.append(m)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "-OgPu3wM-l2F",
        "outputId": "c09137ba-6185-46d2-cfaf-5adf9ad6cce3"
      },
      "source": [
        "idx_rows = ['3 Days', '10 Days', '30 Days', '90 Days']\n",
        "data = {\n",
        "    'MAE':  [metrics[0][0], metrics[1][0], metrics[2][0], metrics[3][0]],\n",
        "    'R2 Score': [metrics[0][1], metrics[1][1], metrics[2][1], metrics[3][1]]\n",
        "}\n",
        "results = pd.DataFrame(data, columns = ['MAE','R2 Score'], index = idx_rows)\n",
        "results"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3 Days</th>\n",
              "      <td>177.979900</td>\n",
              "      <td>0.995668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10 Days</th>\n",
              "      <td>181.998261</td>\n",
              "      <td>0.995152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30 Days</th>\n",
              "      <td>176.394733</td>\n",
              "      <td>0.995825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90 Days</th>\n",
              "      <td>179.174810</td>\n",
              "      <td>0.995381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                MAE  R2 Score\n",
              "3 Days   177.979900  0.995668\n",
              "10 Days  181.998261  0.995152\n",
              "30 Days  176.394733  0.995825\n",
              "90 Days  179.174810  0.995381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_htHgeP5S-"
      },
      "source": [
        "## Downscaling the Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj00M_-6Q8YV"
      },
      "source": [
        "Our current price prediction model uses 90 days of historical prices to predict the next-day price, creating a memory-intensive operation given both the number of rows and columns in the training dataste. Furthermore, we know that days far before the next day are likely to be less significant that days closer to it. As a result, we can apply some dimensionality reduction techniques and see whether or not our optimised regression model still maintains its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87w0X-X6AbIY"
      },
      "source": [
        "X = btc_hist_r.iloc[:, :-1]\n",
        "y = btc_hist_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eek8rlktDyeI"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "luYj_LL0Acjr",
        "outputId": "ebb69e68-8e9b-4fd0-b7d3-0eb14430da01"
      },
      "source": [
        "pca = PCA()\n",
        "pca = pca.fit(X_train)\n",
        "var = pca.explained_variance_\n",
        "num = pca.n_features_\n",
        "names = [str(i) for i in list(range(1, num + 1))]\n",
        "cluster.screeplot(obj = [names, pca.explained_variance_ratio_], dim = (18, 5), show = True)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hyeGVVWDY0m"
      },
      "source": [
        "pca = PCA(n_components = 5)\n",
        "pca = pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O87p_PKwD7xJ"
      },
      "source": [
        "reg = RandomForestRegressor(n_estimators = 200, max_depth = None,\n",
        "                              max_features = 'sqrt')\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjYZpMG8D7xJ",
        "outputId": "d952ee26-40ff-4e79-b965-71146c92b8ff"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "rf_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))\n",
        "rf_metrics"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165.74214781713158, 0.9963804351128439)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zwdw2jHQe5B"
      },
      "source": [
        "## Extending to Other Cryptocurrencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJcOvl_LQhjT"
      },
      "source": [
        "So far, our experiments have been limited to Bitcoin. We can now test and see whether or not our closing price prediction model works well for other crypotcurrencies, and if not, whether its architecture works well if trained and fitted on the data for another cryptocurrency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFOlcnzmFYkc"
      },
      "source": [
        "eth_hist = pd.read_csv('Ethereum-Historical.csv')\n",
        "eth_hist = eth_hist.drop(['SNo', 'Date'], axis = 1)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "i7xl1CTxFYkq",
        "outputId": "ea897d27-ffcd-420b-e56d-fbae77420473"
      },
      "source": [
        "eth_hist_r = eth_hist\n",
        "eth_hist.head()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close1</th>\n",
              "      <th>Close2</th>\n",
              "      <th>Close3</th>\n",
              "      <th>Close4</th>\n",
              "      <th>Close5</th>\n",
              "      <th>Close6</th>\n",
              "      <th>Close7</th>\n",
              "      <th>Close8</th>\n",
              "      <th>Close9</th>\n",
              "      <th>Close10</th>\n",
              "      <th>Close11</th>\n",
              "      <th>Close12</th>\n",
              "      <th>Close13</th>\n",
              "      <th>Close14</th>\n",
              "      <th>Close15</th>\n",
              "      <th>Close16</th>\n",
              "      <th>Close17</th>\n",
              "      <th>Close18</th>\n",
              "      <th>Close19</th>\n",
              "      <th>Close20</th>\n",
              "      <th>Close21</th>\n",
              "      <th>Close22</th>\n",
              "      <th>Close23</th>\n",
              "      <th>Close24</th>\n",
              "      <th>Close25</th>\n",
              "      <th>Close26</th>\n",
              "      <th>Close27</th>\n",
              "      <th>Close28</th>\n",
              "      <th>Close29</th>\n",
              "      <th>Close30</th>\n",
              "      <th>Close31</th>\n",
              "      <th>Close32</th>\n",
              "      <th>Close33</th>\n",
              "      <th>Close34</th>\n",
              "      <th>Close35</th>\n",
              "      <th>Close36</th>\n",
              "      <th>Close37</th>\n",
              "      <th>Close38</th>\n",
              "      <th>Close39</th>\n",
              "      <th>Close40</th>\n",
              "      <th>...</th>\n",
              "      <th>Close52</th>\n",
              "      <th>Close53</th>\n",
              "      <th>Close54</th>\n",
              "      <th>Close55</th>\n",
              "      <th>Close56</th>\n",
              "      <th>Close57</th>\n",
              "      <th>Close58</th>\n",
              "      <th>Close59</th>\n",
              "      <th>Close60</th>\n",
              "      <th>Close61</th>\n",
              "      <th>Close62</th>\n",
              "      <th>Close63</th>\n",
              "      <th>Close64</th>\n",
              "      <th>Close65</th>\n",
              "      <th>Close66</th>\n",
              "      <th>Close67</th>\n",
              "      <th>Close68</th>\n",
              "      <th>Close69</th>\n",
              "      <th>Close70</th>\n",
              "      <th>Close71</th>\n",
              "      <th>Close72</th>\n",
              "      <th>Close73</th>\n",
              "      <th>Close74</th>\n",
              "      <th>Close75</th>\n",
              "      <th>Close76</th>\n",
              "      <th>Close77</th>\n",
              "      <th>Close78</th>\n",
              "      <th>Close79</th>\n",
              "      <th>Close80</th>\n",
              "      <th>Close81</th>\n",
              "      <th>Close82</th>\n",
              "      <th>Close83</th>\n",
              "      <th>Close84</th>\n",
              "      <th>Close85</th>\n",
              "      <th>Close86</th>\n",
              "      <th>Close87</th>\n",
              "      <th>Close88</th>\n",
              "      <th>Close89</th>\n",
              "      <th>Close90</th>\n",
              "      <th>Close91</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.753325</td>\n",
              "      <td>0.701897</td>\n",
              "      <td>0.708448</td>\n",
              "      <td>1.06786</td>\n",
              "      <td>1.21744</td>\n",
              "      <td>1.82767</td>\n",
              "      <td>1.82787</td>\n",
              "      <td>1.68890</td>\n",
              "      <td>1.56603</td>\n",
              "      <td>1.20361</td>\n",
              "      <td>1.08705</td>\n",
              "      <td>1.25886</td>\n",
              "      <td>1.46492</td>\n",
              "      <td>1.39529</td>\n",
              "      <td>1.37923</td>\n",
              "      <td>1.35259</td>\n",
              "      <td>1.23127</td>\n",
              "      <td>1.14019</td>\n",
              "      <td>1.15998</td>\n",
              "      <td>1.14770</td>\n",
              "      <td>1.19138</td>\n",
              "      <td>1.18255</td>\n",
              "      <td>1.31927</td>\n",
              "      <td>1.35824</td>\n",
              "      <td>1.35161</td>\n",
              "      <td>1.29479</td>\n",
              "      <td>1.26493</td>\n",
              "      <td>1.27441</td>\n",
              "      <td>1.33881</td>\n",
              "      <td>1.29583</td>\n",
              "      <td>1.246650</td>\n",
              "      <td>1.242430</td>\n",
              "      <td>1.206510</td>\n",
              "      <td>1.165770</td>\n",
              "      <td>0.982978</td>\n",
              "      <td>1.038740</td>\n",
              "      <td>0.936003</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.944410</td>\n",
              "      <td>0.907175</td>\n",
              "      <td>...</td>\n",
              "      <td>0.582886</td>\n",
              "      <td>0.661146</td>\n",
              "      <td>0.738644</td>\n",
              "      <td>0.690215</td>\n",
              "      <td>0.678574</td>\n",
              "      <td>0.687171</td>\n",
              "      <td>0.668379</td>\n",
              "      <td>0.628643</td>\n",
              "      <td>0.650645</td>\n",
              "      <td>0.609388</td>\n",
              "      <td>0.621716</td>\n",
              "      <td>0.650628</td>\n",
              "      <td>0.627857</td>\n",
              "      <td>0.634963</td>\n",
              "      <td>0.626030</td>\n",
              "      <td>0.607655</td>\n",
              "      <td>0.522968</td>\n",
              "      <td>0.561878</td>\n",
              "      <td>0.536495</td>\n",
              "      <td>0.547178</td>\n",
              "      <td>0.517734</td>\n",
              "      <td>0.489014</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>0.447329</td>\n",
              "      <td>0.567702</td>\n",
              "      <td>0.539657</td>\n",
              "      <td>0.563590</td>\n",
              "      <td>0.616039</td>\n",
              "      <td>0.731317</td>\n",
              "      <td>0.869641</td>\n",
              "      <td>1.002480</td>\n",
              "      <td>1.206660</td>\n",
              "      <td>1.041220</td>\n",
              "      <td>0.916627</td>\n",
              "      <td>1.055670</td>\n",
              "      <td>0.989789</td>\n",
              "      <td>1.013360</td>\n",
              "      <td>0.899050</td>\n",
              "      <td>0.895637</td>\n",
              "      <td>0.926032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.701897</td>\n",
              "      <td>0.708448</td>\n",
              "      <td>1.067860</td>\n",
              "      <td>1.21744</td>\n",
              "      <td>1.82767</td>\n",
              "      <td>1.82787</td>\n",
              "      <td>1.68890</td>\n",
              "      <td>1.56603</td>\n",
              "      <td>1.20361</td>\n",
              "      <td>1.08705</td>\n",
              "      <td>1.25886</td>\n",
              "      <td>1.46492</td>\n",
              "      <td>1.39529</td>\n",
              "      <td>1.37923</td>\n",
              "      <td>1.35259</td>\n",
              "      <td>1.23127</td>\n",
              "      <td>1.14019</td>\n",
              "      <td>1.15998</td>\n",
              "      <td>1.14770</td>\n",
              "      <td>1.19138</td>\n",
              "      <td>1.18255</td>\n",
              "      <td>1.31927</td>\n",
              "      <td>1.35824</td>\n",
              "      <td>1.35161</td>\n",
              "      <td>1.29479</td>\n",
              "      <td>1.26493</td>\n",
              "      <td>1.27441</td>\n",
              "      <td>1.33881</td>\n",
              "      <td>1.29583</td>\n",
              "      <td>1.24665</td>\n",
              "      <td>1.242430</td>\n",
              "      <td>1.206510</td>\n",
              "      <td>1.165770</td>\n",
              "      <td>0.982978</td>\n",
              "      <td>1.038740</td>\n",
              "      <td>0.936003</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.944410</td>\n",
              "      <td>0.907175</td>\n",
              "      <td>0.874231</td>\n",
              "      <td>...</td>\n",
              "      <td>0.661146</td>\n",
              "      <td>0.738644</td>\n",
              "      <td>0.690215</td>\n",
              "      <td>0.678574</td>\n",
              "      <td>0.687171</td>\n",
              "      <td>0.668379</td>\n",
              "      <td>0.628643</td>\n",
              "      <td>0.650645</td>\n",
              "      <td>0.609388</td>\n",
              "      <td>0.621716</td>\n",
              "      <td>0.650628</td>\n",
              "      <td>0.627857</td>\n",
              "      <td>0.634963</td>\n",
              "      <td>0.626030</td>\n",
              "      <td>0.607655</td>\n",
              "      <td>0.522968</td>\n",
              "      <td>0.561878</td>\n",
              "      <td>0.536495</td>\n",
              "      <td>0.547178</td>\n",
              "      <td>0.517734</td>\n",
              "      <td>0.489014</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>0.447329</td>\n",
              "      <td>0.567702</td>\n",
              "      <td>0.539657</td>\n",
              "      <td>0.563590</td>\n",
              "      <td>0.616039</td>\n",
              "      <td>0.731317</td>\n",
              "      <td>0.869641</td>\n",
              "      <td>1.002480</td>\n",
              "      <td>1.206660</td>\n",
              "      <td>1.041220</td>\n",
              "      <td>0.916627</td>\n",
              "      <td>1.055670</td>\n",
              "      <td>0.989789</td>\n",
              "      <td>1.013360</td>\n",
              "      <td>0.899050</td>\n",
              "      <td>0.895637</td>\n",
              "      <td>0.926032</td>\n",
              "      <td>0.927974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.708448</td>\n",
              "      <td>1.067860</td>\n",
              "      <td>1.217440</td>\n",
              "      <td>1.82767</td>\n",
              "      <td>1.82787</td>\n",
              "      <td>1.68890</td>\n",
              "      <td>1.56603</td>\n",
              "      <td>1.20361</td>\n",
              "      <td>1.08705</td>\n",
              "      <td>1.25886</td>\n",
              "      <td>1.46492</td>\n",
              "      <td>1.39529</td>\n",
              "      <td>1.37923</td>\n",
              "      <td>1.35259</td>\n",
              "      <td>1.23127</td>\n",
              "      <td>1.14019</td>\n",
              "      <td>1.15998</td>\n",
              "      <td>1.14770</td>\n",
              "      <td>1.19138</td>\n",
              "      <td>1.18255</td>\n",
              "      <td>1.31927</td>\n",
              "      <td>1.35824</td>\n",
              "      <td>1.35161</td>\n",
              "      <td>1.29479</td>\n",
              "      <td>1.26493</td>\n",
              "      <td>1.27441</td>\n",
              "      <td>1.33881</td>\n",
              "      <td>1.29583</td>\n",
              "      <td>1.24665</td>\n",
              "      <td>1.24243</td>\n",
              "      <td>1.206510</td>\n",
              "      <td>1.165770</td>\n",
              "      <td>0.982978</td>\n",
              "      <td>1.038740</td>\n",
              "      <td>0.936003</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.944410</td>\n",
              "      <td>0.907175</td>\n",
              "      <td>0.874231</td>\n",
              "      <td>0.853685</td>\n",
              "      <td>...</td>\n",
              "      <td>0.738644</td>\n",
              "      <td>0.690215</td>\n",
              "      <td>0.678574</td>\n",
              "      <td>0.687171</td>\n",
              "      <td>0.668379</td>\n",
              "      <td>0.628643</td>\n",
              "      <td>0.650645</td>\n",
              "      <td>0.609388</td>\n",
              "      <td>0.621716</td>\n",
              "      <td>0.650628</td>\n",
              "      <td>0.627857</td>\n",
              "      <td>0.634963</td>\n",
              "      <td>0.626030</td>\n",
              "      <td>0.607655</td>\n",
              "      <td>0.522968</td>\n",
              "      <td>0.561878</td>\n",
              "      <td>0.536495</td>\n",
              "      <td>0.547178</td>\n",
              "      <td>0.517734</td>\n",
              "      <td>0.489014</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>0.447329</td>\n",
              "      <td>0.567702</td>\n",
              "      <td>0.539657</td>\n",
              "      <td>0.563590</td>\n",
              "      <td>0.616039</td>\n",
              "      <td>0.731317</td>\n",
              "      <td>0.869641</td>\n",
              "      <td>1.002480</td>\n",
              "      <td>1.206660</td>\n",
              "      <td>1.041220</td>\n",
              "      <td>0.916627</td>\n",
              "      <td>1.055670</td>\n",
              "      <td>0.989789</td>\n",
              "      <td>1.013360</td>\n",
              "      <td>0.899050</td>\n",
              "      <td>0.895637</td>\n",
              "      <td>0.926032</td>\n",
              "      <td>0.927974</td>\n",
              "      <td>1.027470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.067860</td>\n",
              "      <td>1.217440</td>\n",
              "      <td>1.827670</td>\n",
              "      <td>1.82787</td>\n",
              "      <td>1.68890</td>\n",
              "      <td>1.56603</td>\n",
              "      <td>1.20361</td>\n",
              "      <td>1.08705</td>\n",
              "      <td>1.25886</td>\n",
              "      <td>1.46492</td>\n",
              "      <td>1.39529</td>\n",
              "      <td>1.37923</td>\n",
              "      <td>1.35259</td>\n",
              "      <td>1.23127</td>\n",
              "      <td>1.14019</td>\n",
              "      <td>1.15998</td>\n",
              "      <td>1.14770</td>\n",
              "      <td>1.19138</td>\n",
              "      <td>1.18255</td>\n",
              "      <td>1.31927</td>\n",
              "      <td>1.35824</td>\n",
              "      <td>1.35161</td>\n",
              "      <td>1.29479</td>\n",
              "      <td>1.26493</td>\n",
              "      <td>1.27441</td>\n",
              "      <td>1.33881</td>\n",
              "      <td>1.29583</td>\n",
              "      <td>1.24665</td>\n",
              "      <td>1.24243</td>\n",
              "      <td>1.20651</td>\n",
              "      <td>1.165770</td>\n",
              "      <td>0.982978</td>\n",
              "      <td>1.038740</td>\n",
              "      <td>0.936003</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.944410</td>\n",
              "      <td>0.907175</td>\n",
              "      <td>0.874231</td>\n",
              "      <td>0.853685</td>\n",
              "      <td>0.882391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690215</td>\n",
              "      <td>0.678574</td>\n",
              "      <td>0.687171</td>\n",
              "      <td>0.668379</td>\n",
              "      <td>0.628643</td>\n",
              "      <td>0.650645</td>\n",
              "      <td>0.609388</td>\n",
              "      <td>0.621716</td>\n",
              "      <td>0.650628</td>\n",
              "      <td>0.627857</td>\n",
              "      <td>0.634963</td>\n",
              "      <td>0.626030</td>\n",
              "      <td>0.607655</td>\n",
              "      <td>0.522968</td>\n",
              "      <td>0.561878</td>\n",
              "      <td>0.536495</td>\n",
              "      <td>0.547178</td>\n",
              "      <td>0.517734</td>\n",
              "      <td>0.489014</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>0.447329</td>\n",
              "      <td>0.567702</td>\n",
              "      <td>0.539657</td>\n",
              "      <td>0.563590</td>\n",
              "      <td>0.616039</td>\n",
              "      <td>0.731317</td>\n",
              "      <td>0.869641</td>\n",
              "      <td>1.002480</td>\n",
              "      <td>1.206660</td>\n",
              "      <td>1.041220</td>\n",
              "      <td>0.916627</td>\n",
              "      <td>1.055670</td>\n",
              "      <td>0.989789</td>\n",
              "      <td>1.013360</td>\n",
              "      <td>0.899050</td>\n",
              "      <td>0.895637</td>\n",
              "      <td>0.926032</td>\n",
              "      <td>0.927974</td>\n",
              "      <td>1.027470</td>\n",
              "      <td>0.999278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.217440</td>\n",
              "      <td>1.827670</td>\n",
              "      <td>1.827870</td>\n",
              "      <td>1.68890</td>\n",
              "      <td>1.56603</td>\n",
              "      <td>1.20361</td>\n",
              "      <td>1.08705</td>\n",
              "      <td>1.25886</td>\n",
              "      <td>1.46492</td>\n",
              "      <td>1.39529</td>\n",
              "      <td>1.37923</td>\n",
              "      <td>1.35259</td>\n",
              "      <td>1.23127</td>\n",
              "      <td>1.14019</td>\n",
              "      <td>1.15998</td>\n",
              "      <td>1.14770</td>\n",
              "      <td>1.19138</td>\n",
              "      <td>1.18255</td>\n",
              "      <td>1.31927</td>\n",
              "      <td>1.35824</td>\n",
              "      <td>1.35161</td>\n",
              "      <td>1.29479</td>\n",
              "      <td>1.26493</td>\n",
              "      <td>1.27441</td>\n",
              "      <td>1.33881</td>\n",
              "      <td>1.29583</td>\n",
              "      <td>1.24665</td>\n",
              "      <td>1.24243</td>\n",
              "      <td>1.20651</td>\n",
              "      <td>1.16577</td>\n",
              "      <td>0.982978</td>\n",
              "      <td>1.038740</td>\n",
              "      <td>0.936003</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.944410</td>\n",
              "      <td>0.907175</td>\n",
              "      <td>0.874231</td>\n",
              "      <td>0.853685</td>\n",
              "      <td>0.882391</td>\n",
              "      <td>0.938445</td>\n",
              "      <td>...</td>\n",
              "      <td>0.678574</td>\n",
              "      <td>0.687171</td>\n",
              "      <td>0.668379</td>\n",
              "      <td>0.628643</td>\n",
              "      <td>0.650645</td>\n",
              "      <td>0.609388</td>\n",
              "      <td>0.621716</td>\n",
              "      <td>0.650628</td>\n",
              "      <td>0.627857</td>\n",
              "      <td>0.634963</td>\n",
              "      <td>0.626030</td>\n",
              "      <td>0.607655</td>\n",
              "      <td>0.522968</td>\n",
              "      <td>0.561878</td>\n",
              "      <td>0.536495</td>\n",
              "      <td>0.547178</td>\n",
              "      <td>0.517734</td>\n",
              "      <td>0.489014</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>0.447329</td>\n",
              "      <td>0.567702</td>\n",
              "      <td>0.539657</td>\n",
              "      <td>0.563590</td>\n",
              "      <td>0.616039</td>\n",
              "      <td>0.731317</td>\n",
              "      <td>0.869641</td>\n",
              "      <td>1.002480</td>\n",
              "      <td>1.206660</td>\n",
              "      <td>1.041220</td>\n",
              "      <td>0.916627</td>\n",
              "      <td>1.055670</td>\n",
              "      <td>0.989789</td>\n",
              "      <td>1.013360</td>\n",
              "      <td>0.899050</td>\n",
              "      <td>0.895637</td>\n",
              "      <td>0.926032</td>\n",
              "      <td>0.927974</td>\n",
              "      <td>1.027470</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.934348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Close1    Close2    Close3  ...   Close89   Close90   Close91\n",
              "0  0.753325  0.701897  0.708448  ...  0.899050  0.895637  0.926032\n",
              "1  0.701897  0.708448  1.067860  ...  0.895637  0.926032  0.927974\n",
              "2  0.708448  1.067860  1.217440  ...  0.926032  0.927974  1.027470\n",
              "3  1.067860  1.217440  1.827670  ...  0.927974  1.027470  0.999278\n",
              "4  1.217440  1.827670  1.827870  ...  1.027470  0.999278  0.934348\n",
              "\n",
              "[5 rows x 91 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESgSh4wVFvBj"
      },
      "source": [
        "### Existing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N20NkPMpFYkr"
      },
      "source": [
        "X = eth_hist_r.iloc[:, :-1]\n",
        "y = eth_hist_r.iloc[:, -1]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLcRaqp1GGDE"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5u029RgFzgt",
        "outputId": "ea5054c7-151e-4c08-b714-8c8298183363"
      },
      "source": [
        "y_pred = reg_historical_opt.predict(X)\n",
        "rf_metrics = (mean_absolute_error(y, y_pred), \n",
        "              r2_score(y, y_pred))\n",
        "rf_metrics"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5154.498708450041, -654.9813288259772)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utcJegXEGSKl"
      },
      "source": [
        "### Existing Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4L3Fe3SGcpQ"
      },
      "source": [
        "X = eth_hist_r.iloc[:, :-1]\n",
        "y = eth_hist_r.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLEWlfa9GcpQ"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYcCaUQpGcpR"
      },
      "source": [
        "reg = RandomForestRegressor(n_estimators = 200, max_depth = None,\n",
        "                            max_features = 'sqrt')\n",
        "reg = reg.fit(X_train, y_train)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyaHto5LGmsM",
        "outputId": "bb917a82-9e33-415f-deec-5222e3fb3d06"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "rf_metrics = (mean_absolute_error(y_test, y_pred), \n",
        "              r2_score(y_test, y_pred))\n",
        "rf_metrics"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11.87864444672022, 0.9937325653238789)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    }
  ]
}